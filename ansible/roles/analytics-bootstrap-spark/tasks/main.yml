## Bootstrap for spark ##

- name: Adding ENV Vars to bashrc file of spark.
  become: yes
  become_user: "{{ analytics_user }}"
  lineinfile:
    path: '{{ analytics_user_home }}/.bashrc'
    line: 'export {{item.var}}={{item.value}}'
    regexp: "export {{ item.var }}.*"
  with_items:
    - {var: 'AWS_ACCESS_KEY_ID', value: '{{ aws_access_key_id }}'}
    - {var: 'AWS_SECRET_ACCESS_KEY', value: '{{ aws_secret_access_key }}'}

- name: Adding ENV Vars to spark servers environment.
  become: yes
  lineinfile:
    path: '/etc/environment'
    line: '{{item.var}}={{item.value}}'
    regexp: "{{ item.var }}.*"
  with_items:
    - {var: 'AWS_ACCESS_KEY_ID', value: '{{ aws_access_key_id }}'}
    - {var: 'AWS_SECRET_ACCESS_KEY', value: '{{ aws_secret_access_key }}'}
    - {var: 'AWS_REGION', value: '{{ aws_region }}'}
    - {var: 'AWS_PRIVATE_S3_BUCKET', value: '{{ aws_private_s3_bucket }}'}
    - {var: 'AWS_PUBLIC_S3_BUCKET', value: '{{ aws_public_s3_bucket }}'}
    - {var: 'REPORT_BACKUP_CONTAINER', value: 'portal-reports-backup'}
    - {var: 'GOOGLE_CREDENTIALS_PATH', value: '/home/analytics/credentials'}
    - {var: 'STORAGE_PROVIDER', value: 'AZURE'}
    - {var: 'ENV', value: '{{env}}'}
    - {var: 'KAFKA_BROKER_HOST', value: "{{groups['processing-cluster-kafka'][0]}}:9092"}

- name: Install required python packages
  become: yes
  action: apt pkg={{ item }} state=present update_cache=yes
  with_items:
    - libffi-dev
    - libssl-dev
    - build-essential
    - lzop
    - curl

- name: Install libraries for spark bootstrap
  become: yes
  action: apt pkg={{ item }} state=present update_cache=yes
  with_items:
    - build-essential
    - git

- name: Create directories for spark/data-products
  become: yes
  file: path={{ item }} owner={{ analytics_user }} group={{ analytics_group }} state=directory
  with_items: "{{ analytics.paths }}"
